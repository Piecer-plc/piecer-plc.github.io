[1:](https://www.kaggle.com/code/abdulaziz04/simple-approach-using-xgbclassifier/comments) oh wow!, do you know any reason as to why different versions act differently and can we replicate such results by tuning hyperparameters of a given version? Also this score is on the training set right?

[2:](https://www.kaggle.com/code/akioonodera/tps-mar2022-lgbm-regression/comments) Great experiment.I was surprised that the runtimes were so different.

[3:](https://www.kaggle.com/code/alexandershumilin/tabular-playground-series-aug-2022-nn-keras/comments#2011102) This is a very interesting observation!

[4: ](https://www.kaggle.com/code/alfamame/tps-may-2022-ann/comments)I am a newbie and appreciate your thought-provoking input!

[5:](https://www.kaggle.com/code/aniruddhapa/insurance-claim/comments) Happy to hear that it helped. Keep going.

[6:](https://www.kaggle.com/code/bennyfung/tps-dec-keras-nn-score-0-95586/comments#2011217) It is interesting that version 2.4.1 is better than 2.7.0. Also, the memory consumption of 2.7.0 is double of 2.4.1.

[7:](https://www.kaggle.com/code/bibhabasumohapatra/vanilla-random-forest-with-kfolds/comments) Did not know thatâ€¦. So cool!!! Thanks for choosing my nb and publishing the results.

[8:](https://www.kaggle.com/code/faridtaghiyev/pawpularity-regression-on-several-models/comments#2014375) Cool, I didn't even know that. Appreciate your work !

[9:](https://www.kaggle.com/code/gopalgoyal612002/tps-aug-xgboost-classifier-eda/comments) ohh, good to know

[10:](https://www.kaggle.com/code/hussainsalih/spaceship-titanic-model/comments) your welcome 

[11:](https://www.kaggle.com/code/japancolorado/introduction-and-tabular-playground-notebook/comments) Interesting! I hadn't considered the role of xgboost version on memory usage! Thanks for pointing that out!

[12:](https://www.kaggle.com/code/joshxmiller656/sklearn-catboost/comments) Thanks! Really interesting analysis. Cool to see that catboost has gotten faster over time! Variations look pretty small, but I guess these are adjustments to the training algorithm over time that make the difference.

[13:](https://www.kaggle.com/code/lucamassaron/lightgbm-with-multiclass-focal-loss/comments) that's extremely interesting as an experiment! From a data scientist point of view, reproducibility of the score across multiple versions of packages and different runs is indeed paramount!

[14:](https://www.kaggle.com/code/mmellinger66/functional-tensorflow-keras-starter-notebook/comments#2014345) Is it really worth using an older version of TensorFlow?

[15:](https://www.kaggle.com/code/nafishamoin/tabular-playground-november-2021/comments) thank you as well for you're interest in the solution :)

[16:](https://www.kaggle.com/code/phuc16102001/tps-2022-xgboost/comments) Great! Thank you for your sharing.

[17:](https://www.kaggle.com/code/ranjeetshrivastav/ventilator-pressure-prediction-xgboost/comments) Thanks for this valuable information, I will surely try and compare different versions.

[18:](https://www.kaggle.com/code/salama4ai/salama4ai-tabular-playground-series-oct-2021/comments#1988659) I highly appreciate your effort and the valuable information you shared, i can't be more appreciate

[19:](https://www.kaggle.com/code/sergeyzemskov/tps-08-21-cross-val-catboost-eda/comments) Thanks for your research, interesting results. I think the reseat of slowing that new versions of catboos have more conditions and validations

[20:](https://www.kaggle.com/code/simonhm/data-analyze-and-logistic-regression/comments) Thanks for letting me know.

[21:](https://www.kaggle.com/code/stpeteishii/tps0222-visualize-importance/comments) Thank you for letting me know the valuable information.

[22:](https://www.kaggle.com/code/tatyy555/tps-feb-2022-bacteria-species-keras-conv1d/comments) Thank you for your insight!!

[23:](https://www.kaggle.com/code/yasseinmahmoud/eda-feature-engineering-nn-prediction-may-2022/comments) Thanks alot for your notice, I didn't take this tip in consideration while training this model, and thanks for sharing your models' benchmarks.


















































































































































































































































































